\documentclass[preprint,12pt]{elsarticle}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float} % For [H] float placement

\journal{Artificial Vision Journal}

\begin{document}

\begin{frontmatter}

\title{Artificial Vision Techniques for Agricultural Diagnosis of Soybean}

\author[aff1]{Jeisson Esteban Sánchez Robles\corref{cor1}}
\ead{u20221202900@usco.edu.co}

\author[aff1]{Juan Andrés Vidarte Dussan}
\ead{u20221203011@usco.edu.co}

\cortext[cor1]{Corresponding author}

\affiliation[aff1]{
    organization={University of Southern Colombia, Department of Computer Science},
    addressline={Carrera 1, Calle 9},
    city={Neiva},
    postcode={410001},
    state={Huila},
    country={Colombia}
}

\begin{abstract}
  This research presents AGROAI, an intelligent system designed to detect and classify diseases in soybean crops using computer vision and deep learning techniques. The project addresses the critical need for timely and accurate disease identification in agriculture, which directly impacts crop yield and sustainability. The methodology integrates a large dataset of annotated soybean leaf images, representing seven common diseases and healthy leaves, and leverages Vision Transformer (ViT) architectures for automated image classification. The system was trained and validated using a dataset split into 70\% for training, 15\% for validation, and 15\% for testing. The model achieved high accuracy in detecting diseases such as bacterial blight, Cercospora leaf spot, downy mildew, and others, demonstrating the effectiveness of Vision Transformers in agricultural contexts. The application generates early diagnoses and actionable recommendations to support farmers in making informed decisions. The results indicate that the proposed approach can significantly reduce diagnostic time and increase the reliability of disease detection, contributing to improved crop management and food security. Future work will focus on expanding the model's capabilities to other crops and integrating mobile deployment for real-time field usage.
\end{abstract}
  
\begin{keyword}
  Artificial Vision \sep Soybean \sep Plant Diseases \sep Deep Learning \sep Vision Transformer \sep Image Classification \sep Precision Agriculture
  \end{keyword}  

\end{frontmatter}

\section{Introduction}

Soybean is one of the world's most important crops due to its nutritional value and widespread use in food, animal feed, and industrial products. However, soybean production is highly susceptible to a range of plant diseases that can significantly reduce crop yields and affect food security. Among the most common diseases affecting soybean crops are bacterial blight, Cercospora leaf spot, downy mildew, frog-eye leaf spot, soybean rust, target spot, and potassium deficiency, all of which exhibit distinct symptoms on the leaves.

Traditionally, disease diagnosis has relied on manual inspection by agricultural experts. While effective, this process is time-consuming, prone to human error, and often unfeasible in large-scale farming operations. In recent years, the integration of Artificial Intelligence (AI), particularly in the field of computer vision, has emerged as a promising solution for automated plant disease detection. Deep learning models, especially Convolutional Neural Networks (CNNs), have demonstrated strong performance in image-based classification tasks. However, the emergence of Vision Transformers (ViTs) has introduced new opportunities for improving accuracy and robustness in visual diagnosis tasks.

In this research, we propose AGROAI, an intelligent system based on Vision Transformer architectures to classify and detect diseases in soybean leaves. The system processes high-resolution images and provides early and reliable diagnostic feedback. This approach not only reduces the time and cost associated with traditional methods but also enhances precision agriculture practices by delivering actionable insights directly to farmers and agricultural stakeholders.

The following sections describe related work in this field, present the theoretical background, explain the methodology and mathematical model used, and provide experimental results to validate the effectiveness of the proposed solution.

\section{Related Work}

Early efforts in plant disease diagnosis through artificial vision primarily focused on classical image processing techniques, including color segmentation, edge detection, and morphological operations \cite{Patil2011LeafDiseaseDetection}. While these approaches provided initial results, their performance was limited by sensitivity to environmental conditions and hand-crafted feature dependencies.

With the advent of deep learning, Convolutional Neural Networks (CNNs) became the dominant technique in plant disease classification. Mohanty et al. \cite{Mohanty2016UsingDL} demonstrated the effectiveness of CNNs by applying AlexNet and GoogLeNet to a dataset of 54,000 images from 14 plant species, achieving high classification accuracy. Similarly, Sladojevic et al. \cite{Sladojevic2016DeepNN} employed a custom CNN architecture to classify multiple plant diseases with considerable success.

Further advancements include ensemble CNN models. Brahimi et al. \cite{Brahimi2017DeepLF} proposed an ensemble approach for tomato leaf diseases that improved classification robustness. Ferentinos \cite{Ferentinos2018DeepLA} developed a generalized CNN model for 25 different plant species, demonstrating the scalability of deep learning in agriculture. However, these models often require extensive data augmentation and suffer from limited generalization to new environments.

To improve model attention on relevant leaf areas, attention-based mechanisms and hybrid CNN-RNN architectures have been introduced. Fuentes et al. \cite{Fuentes2017RobustDC} used region-based CNNs for tomato disease detection, achieving notable performance improvements. Despite their success, these models introduce complexity and demand significant computational resources.

More recently, Transformer-based architectures have gained traction. Dosovitskiy et al. \cite{Dosovitskiy2020AnII} introduced the Vision Transformer (ViT), which uses self-attention mechanisms and has shown competitive results with CNNs on large datasets. In agriculture, Li et al. \cite{Li2023GrapeViT} applied ViTs to grape leaf disease classification, while Gao et al. \cite{Gao2022WheatRust} proposed a hybrid ViT-CNN model for wheat rust detection, showcasing ViT's adaptability to complex agricultural datasets.

In the context of soybean disease detection, few works leverage Vision Transformers. Prior models predominantly utilize CNNs, such as those proposed by Zhang et al. \cite{Zhang2020SoybeanCNN}, which achieved moderate accuracy but struggled with generalization across lighting and leaf conditions.

Unlike previous studies, our work proposes a comprehensive evaluation of four transformer-based architectures—ViT, Swin Transformer, DeiT, and PVT—trained on a balanced dataset of 35,000 images covering seven soybean diseases and healthy leaves. To the best of our knowledge, this is one of the most extensive comparisons of Vision Transformers in this domain. Our approach not only improves diagnostic accuracy but also supports early detection and practical deployment in the field.

\section{Theoretical Framework}

This section provides a conceptual foundation for understanding the main components involved in the design and development of AGROAI, including soybean crop pathology, artificial vision, deep learning, neural networks, and Vision Transformer architectures.

\subsection{Soybean Crop and Leaf Diseases}

Soybean (\textit{Glycine max}) is a vital crop in global agriculture, widely cultivated for its protein and oil content. However, soybean leaves are susceptible to numerous diseases that compromise yield and quality. Among the most common are bacterial blight, Cercospora leaf spot, downy mildew, frog-eye leaf spot, soybean rust, target spot, and potassium deficiency. These diseases manifest through visual symptoms such as discoloration, lesions, spots, and deformation, making them suitable candidates for image-based classification systems \cite{Ferentinos2018DeepLA}.

\subsection{Artificial Vision in Agriculture}

Artificial vision, also known as computer vision, refers to the use of image processing and pattern recognition to extract meaningful information from visual data. In agriculture, this technology has been adopted to automate inspection tasks such as plant growth monitoring, fruit grading, and disease detection \cite{Mohanty2016UsingDL}. The typical pipeline involves image acquisition, preprocessing, feature extraction, and classification. With the advent of machine learning, manual feature engineering has increasingly been replaced by automatic representation learning through deep models.

\subsection{Deep Learning and Neural Networks}

Deep learning is a subset of machine learning that uses layered architectures—primarily neural networks—to learn hierarchical representations from data. The most commonly used deep learning models for image analysis are Convolutional Neural Networks (CNNs), which exploit spatial locality and parameter sharing to efficiently extract features from images \cite{Sladojevic2016DeepNN}. CNNs are composed of convolutional layers, pooling layers, and fully connected layers. These models have been extensively applied to plant disease detection due to their robustness and high accuracy \cite{Ferentinos2018DeepLA,Brahimi2017DeepLF}.

\subsection{Vision Transformers (ViT)}

Transformers, originally introduced for natural language processing, have been adapted to vision tasks through the Vision Transformer (ViT) architecture \cite{Dosovitskiy2020AnII}. Unlike CNNs, ViTs divide input images into fixed-size patches and treat each patch as a token in a sequence. These tokens are embedded and processed using multi-head self-attention mechanisms, allowing the model to capture global context across the image.

ViTs have shown comparable or superior performance to CNNs when trained on large datasets. Their hierarchical variants, such as the Swin Transformer \cite{Liu2021SwinTH}, and lightweight versions like DeiT \cite{Touvron2021TrainingDI}, offer improvements in efficiency and scalability. The Pyramid Vision Transformer (PVT) further incorporates spatial hierarchies to enhance dense prediction tasks such as object detection and segmentation \cite{Wang2021PyramidVT}. These models are particularly effective in agricultural settings where fine-grained visual details are essential for accurate diagnosis \cite{Li2023GrapeViT,Gao2022WheatRust}.

\subsection{Data Augmentation and Model Generalization}

To improve model generalization and reduce overfitting, data augmentation techniques are commonly used. These include random rotations, flips, brightness adjustments, and noise injection. Such transformations simulate variability found in real-world agricultural images, enabling the model to learn more robust features. Additionally, transfer learning and fine-tuning on pre-trained transformer models further enhance performance when dealing with limited or domain-specific datasets \cite{Touvron2021TrainingDI}.

\subsection{Evaluation Metrics in Classification Tasks}

In the context of disease classification, standard evaluation metrics include accuracy, precision, recall, and F1-score. These metrics provide insights into the model’s ability to correctly identify disease categories and minimize false positives or negatives. Confusion matrices are also widely used to visualize performance across multiple classes. These metrics are essential in agricultural applications where misdiagnosis can lead to economic losses or ineffective treatments \cite{Ferentinos2018DeepLA}.

\section{Methodology}

This research followed a structured pipeline comprising three main phases: dataset construction, data preprocessing, and model implementation using advanced deep learning architectures based on Vision Transformers.

\subsection{Dataset Construction}

The dataset includes 35,000 images of soybean leaves across eight categories: seven disease types (bacterial blight, Cercospora leaf spot, downy mildew, frog-eye leaf spot, soybean rust, target spot, potassium deficiency) and healthy leaves. Data were collected from public repositories such as Mendeley Data, Zenodo, and Kaggle, based on criteria like image resolution (400x400 pixels), recent acquisition (5 years), clear labeling, and diversity of capture conditions.

Images were manually validated and organized into eight folders, each representing a specific class. The dataset was evenly balanced with 4,375 images per class to ensure robust learning during training.

\subsection{Preprocessing and Data Augmentation}

\textbf{1. Initial Review and Cleaning:}
\begin{itemize}
  \item Corrupted and unreadable images were detected and removed.
  \item Duplicate images were identified via MD5 hashing and eliminated.
  \item Images were renamed using class labels and numerical suffixes.
\end{itemize}

\textbf{2. Standardization and Normalization:}
\begin{itemize}
  \item All images were converted to JPEG format in RGB color space.
  \item Each image was resized to 224$\times$224 pixels to match the input size required by Vision Transformer models.
\end{itemize}

\textbf{3. Data Augmentation:} To improve generalization and increase model robustness, the following transformations were applied:
\begin{itemize}
  \item Random rotations (±20 degrees)
  \item Scaling and zooming
  \item Blur adjustments
  \item Horizontal and vertical flipping
\end{itemize}
These augmentations simulate field conditions and increase the diversity of the training data without the need for additional manual labeling. The selection of augmentation strategies was based on recommendations found in comprehensive augmentation studies \cite{Shorten2019Survey,Buslaev2020Albumentations}.

These augmentations were applied using Python libraries such as \texttt{Augmentor} and \texttt{OpenCV}.

\subsection{Dataset Splitting}

The preprocessed dataset was divided into three subsets:
\begin{itemize}
  \item 70\% for training
  \item 15\% for validation
  \item 15\% for testing
\end{itemize}

The split ensured that all classes were proportionally represented in each subset.

\subsection{Model Selection and Architecture}

Four Transformer-based models were implemented:
\begin{enumerate}
  \item \textbf{ViT (Vision Transformer)} – for learning global relationships in the image \cite{Dosovitskiy2020AnII}.
  \item \textbf{DeiT} – optimized for training with fewer samples using knowledge distillation \cite{Touvron2021TrainingDI}.
  \item \textbf{Swin Transformer} – includes shifted windows for hierarchical processing and local context \cite{Liu2021SwinTH}.
  \item \textbf{PVT (Pyramid Vision Transformer)} – combines spatial pyramid representations with transformer blocks for efficient feature extraction \cite{Wang2021PyramidVT}.
\end{enumerate}


Although lightweight convolutional models such as MobileNetV3 \cite{Howard2020MobileNetV3} and EfficientNetV2 \cite{Tan2021Efficientnetv2} have proven effective for resource-constrained environments, their ability to capture long-range dependencies is limited. In contrast, Vision Transformers are better suited for high-resolution agricultural tasks requiring global attention and fine-grained detail extraction.

All models were initialized with ImageNet pre-trained weights and fine-tuned on the soybean dataset using transfer learning.

\subsection{Training Configuration}

Training was conducted using the PyTorch framework \cite{Paszke2019PyTorch}, which provides dynamic computational graphs and CUDA acceleration for efficient deep learning model development.

\begin{itemize}
  \item \textbf{Framework:} PyTorch (v2.0), using CUDA acceleration with NVIDIA RTX 4060 GPU.
  \item \textbf{Loss Function:} Cross-entropy loss.
  \item \textbf{Optimizer:} AdamW with weight decay.
  \item \textbf{Learning Rate:} $1 \times 10^{-4}$, adjusted using cosine annealing.
  \item \textbf{Epochs:} 15, 25 and 35 (models saved every 5 epochs).
  \item \textbf{Batch Size:} 4 and 16.
  \item \textbf{Evaluation:} Conducted using accuracy, precision, recall, F1-score, and confusion matrix.
\end{itemize}

\subsection{Evaluation Metrics}

Each model was evaluated on the unseen test set, using:
\begin{itemize}
  \item \textbf{Accuracy} – Overall classification performance.
  \item \textbf{Precision and Recall} – For class-wise analysis.
  \item \textbf{F1-score} – Harmonic mean of precision and recall.
  \item \textbf{Confusion Matrix} – Visual representation of class-wise predictions.
\end{itemize}

This pipeline ensured a rigorous and replicable approach for evaluating the performance of Vision Transformers on soybean leaf disease detection.

\section{Mathematical Model}

This section describes the mathematical formulation and implementation of AGROAI, a deep learning-based system for classifying soybean leaf images into eight categories (seven diseases and healthy class). The classification task is modeled as a multi-class image classification problem, supported by a robust preprocessing pipeline and a Vision Transformer (ViT) architecture.

\subsection{Dataset Preprocessing}

Let the original dataset be defined as:

\begin{equation}
L = \{I_1, I_2, ..., I_n\}
\label{eq:dataset-definition}
\end{equation}

where \( I_i \) represents an individual image in RGB format and \( n \) is the total number of images.

\subsubsection{Image Integrity Verification}

A verification function is applied to filter valid images:

\begin{equation}
\text{verify}(I_i) =
\begin{cases}
1, & \text{if } I_i \text{ is in a valid format and not corrupted} \\
0, & \text{otherwise}
\end{cases}
\label{eq:image-verification}
\end{equation}

The valid image subset becomes:

\begin{equation}
L_{\text{val}} = \{ I_i \in L \mid \text{verify}(I_i) = 1 \}
\label{eq:valid-images}
\end{equation}

\subsubsection{Duplicate Image Removal}

To eliminate duplicates, a hash function is applied:

\begin{equation}
H(I_i) = \text{MD5}(I_i)
\label{eq:hash-function}
\end{equation}

Duplicate detection is performed as:

\begin{equation}
H(I_i) = H(I_j) \Rightarrow I_i \equiv I_j \quad (i \ne j)
\label{eq:duplicate-detection}
\end{equation}

The resulting unique dataset is:

\begin{equation}
L_{\text{uniq}} = \{ I_i \in L_{\text{val}} \mid H(I_i) \notin H \}
\label{eq:unique-dataset}
\end{equation}

\subsubsection{Format Conversion and Renaming}

Each image is converted to RGB and saved as JPEG:

\begin{equation}
I_i' = \text{convert}_{\text{RGB} \rightarrow \text{JPEG}}(I_i)
\label{eq:format-conversion}
\end{equation}

Renaming convention:

\begin{equation}
\text{name}(I_i') = \text{class}_i\_\text{index}.jpg
\label{eq:naming}
\end{equation}

\subsubsection{Image Resizing}

All images are resized to \( 224 \times 224 \) pixels:

\begin{equation}
g(I, m', n') \rightarrow I'
\label{eq:image-resize}
\end{equation}

where \( g \) is the transformation function, and \( m' \), \( n' \) are the new dimensions.

\subsection{Data Augmentation}

Data augmentation generates variations of the original dataset using transformations such as rotation, zoom, and blur:

\begin{equation}
L_{\text{aug}} = h(I, \sigma) \rightarrow I'
\label{eq:data-augmentation}
\end{equation}

where \( \sigma \) includes parameters like rotation angles and zoom factors.

\subsection{Image Representation}

Each image is treated as a matrix:

\begin{equation}
\hat{I} = [i_{m,n}]
\label{eq:image-matrix}
\end{equation}

where \( i_{m,n} \in [0,255] \) represents pixel intensities in the RGB channels.

\subsection{Vision Transformer (ViT) Architecture}

\subsubsection{Patch Embedding}

An image is divided into non-overlapping patches of size \( P \times P \), resulting in:

\begin{equation}
N = \frac{H \cdot W}{P^2}
\label{eq:num-patches}
\end{equation}

Each patch is flattened and projected into a latent space:

\begin{equation}
x_p^i \in \mathbb{R}^{P^2 \cdot 3} \rightarrow z_0^i \in \mathbb{R}^D
\label{eq:patch-projection}
\end{equation}

\subsubsection{Positional Encoding}

To encode spatial information, a positional embedding is added:

\begin{equation}
z_0 = [x_{\text{class}}; z_0^1 + e_1; ...; z_0^N + e_N]
\label{eq:positional-embedding}
\end{equation}

\subsubsection{Transformer Encoder}

\textbf{Multi-head self-attention:}

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V
\label{eq:attention}
\end{equation}

\textbf{MLP Layer:}

\begin{equation}
\text{MLP}(x) = W_2 \cdot \text{GELU}(W_1 \cdot x + b_1) + b_2
\label{eq:mlp}
\end{equation}

\textbf{Residual Connections:}

\begin{equation}
z = z + \text{Attention}, \quad z = z + \text{MLP}
\label{eq:residual}
\end{equation}

\subsubsection{Classification Head}

The final output is taken from the [CLS] token:

\begin{equation}
\hat{y} = \text{softmax}(W_{\text{cls}} \cdot z_{\text{cls}} + b)
\label{eq:classification}
\end{equation}

This results in a vector \( \hat{y} \in \mathbb{R}^8 \), one score for each class.

\subsubsection{Final Model Representation}

The complete ViT-based classification process is defined as:

\begin{equation}
\hat{y} = f(I) = \text{softmax}(\text{Transformer}(\text{Patches}(I) + \text{Position})[\text{CLS}])
\label{eq:vit-final}
\end{equation}

\subsection{Accuracy and Recall}

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
\label{eq:precision-recall}
\end{equation}

Where:
\begin{itemize}
  \item \( TP \): True Positives
  \item \( FP \): False Positives
  \item \( FN \): False Negatives
\end{itemize}

\section{Results and Discussion}

This section presents the performance evaluation of the four Vision Transformer-based models implemented for soybean leaf disease classification: ViT, Swin Transformer, DeiT, and PVT. All models were trained and validated using the same dataset split (70\% training, 15\% validation, 15\% testing) and under identical experimental conditions.

\subsection{Quantitative Results}

Table~\ref{tab:performance} summarizes the classification results for each model based on four standard evaluation metrics: accuracy, precision, recall, and F1-score.

\begin{table}[H]
\centering
\caption{Performance comparison of Vision Transformer models}
\label{tab:performance}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1-score (\%)} \\
\hline
ViT              & 93.2 & 92.8 & 93.0 & 92.9 \\
Swin Transformer & 91.5 & 91.0 & 90.8 & 90.9 \\
DeiT             & 89.8 & 88.9 & 89.1 & 89.0 \\
PVT              & 88.3 & 87.5 & 87.8 & 87.6 \\
\hline
\end{tabular}
\end{table}

\subsection{Analysis and Discussion}

Among the evaluated models, the Vision Transformer (ViT) achieved the highest overall performance, with an accuracy of 93.2\%, outperforming Swin Transformer and DeiT by a margin of 1.7\% and 3.4\% respectively. The ViT also demonstrated superior balance between precision (92.8\%) and recall (93.0\%), resulting in a strong F1-score of 92.9\%.

The Swin Transformer showed competitive results, particularly in precision and recall, which reflects its hierarchical window-based architecture’s capacity to capture both local and global context. This makes it suitable for image classification tasks involving subtle visual differences such as those found in plant leaf diseases.

DeiT, while being computationally efficient and optimized for small datasets, exhibited slightly lower performance, indicating that its lightweight design might compromise feature expressiveness in high-resolution image domains.

PVT achieved the lowest scores across all metrics. Although it is known for effective dense prediction tasks, its relative underperformance in this classification task may be attributed to its focus on spatial pyramid hierarchies rather than fine-grained patch-level attention required in leaf disease classification.

Overall, the results validate the effectiveness of Vision Transformers in agricultural diagnosis tasks. The improvement in model accuracy, especially by ViT, demonstrates the benefit of self-attention mechanisms in capturing long-range dependencies across disease patterns.

\subsection{Model Implications}

The superior performance of ViT suggests that it is the most appropriate architecture for deployment in AGROAI. Its accuracy and generalization capabilities enable reliable disease identification, which can support early intervention and minimize yield losses in soybean cultivation. Additionally, the modular design of ViT allows for adaptation to other crops and plant diseases, broadening the scope of AGROAI’s application.

Future iterations may combine ViT with spatial attention modules or integrate multispectral data to further improve diagnostic precision in real-world agricultural scenarios.

\begin{thebibliography}{99}

\bibitem{Patil2011LeafDiseaseDetection}
J. S. Patil and R. Kumar,
\newblock ``Advances in Image Processing for Detection of Plant Diseases,''
\newblock {\em International Journal of Application or Innovation in Engineering \& Management (IJAIEM)}, vol. 2, no. 11, pp. 168–175, 2011.

\bibitem{Mohanty2016UsingDL}
S. P. Mohanty, D. P. Hughes, and M. Salathé,
\newblock ``Using Deep Learning for Image-Based Plant Disease Detection,''
\newblock {\em Frontiers in Plant Science}, vol. 7, p. 1419, 2016.

\bibitem{Sladojevic2016DeepNN}
S. Sladojevic, M. Arsenovic, A. Anderla, D. Culibrk, and D. Stefanovic,
\newblock ``Deep Neural Networks Based Recognition of Plant Diseases by Leaf Image Classification,''
\newblock {\em Computational Intelligence and Neuroscience}, vol. 2016, Article ID 3289801, 2016.

\bibitem{Brahimi2017DeepLF}
M. Brahimi, K. Boukhalfa, and A. M. Moussaoui,
\newblock ``Deep Learning for Tomato Diseases: Classification and Symptoms Visualization,''
\newblock {\em Applied Artificial Intelligence}, vol. 31, no. 4, pp. 299-315, 2017.

\bibitem{Ferentinos2018DeepLA}
K. P. Ferentinos,
\newblock ``Deep learning models for plant disease detection and diagnosis,''
\newblock {\em Computers and Electronics in Agriculture}, vol. 145, pp. 311-318, 2018.

\bibitem{Fuentes2017RobustDC}
A. Fuentes, S. Yoon, S. C. Kim, and D. S. Park,
\newblock ``A Robust Deep-Learning-Based Detector for Real-Time Tomato Plant Diseases and Pests Recognition,''
\newblock {\em Sensors}, vol. 17, no. 9, p. 2022, 2017.

\bibitem{Dosovitskiy2020AnII}
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
\newblock ``An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,''
\newblock {\em International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{Li2023GrapeViT}
Y. Li, X. Wang, H. Chen, and L. Zhang,
\newblock ``Grape Leaf Disease Identification Based on Vision Transformer,''
\newblock {\em Computers and Electronics in Agriculture}, vol. 205, p. 107618, 2023.

\bibitem{Gao2022WheatRust}
S. Gao, Z. Zhang, and Y. Liu,
\newblock ``A Wheat Rust Detection Model Based on Vision Transformer and Convolutional Neural Network,''
\newblock {\em Computers and Electronics in Agriculture}, vol. 198, p. 107102, 2022.

\bibitem{Zhang2020SoybeanCNN}
S. Zhang, Y. Huang, and T. Pu,
\newblock ``Soybean Leaf Disease Identification Based on CNN with Feature Enhancement,''
\newblock {\em IEEE Access}, vol. 8, pp. 187939–187947, 2020.

\bibitem{Patil2011LeafDiseaseDetection}
J. Patil and R. Kumar,
\newblock ``Advances in Image Processing for Detection of Plant Diseases,''
\newblock {\em Journal of Advanced Bioinformatics Applications and Research}, vol. 2, no. 2, pp. 135--141, 2011.

\bibitem{Brahimi2017DeepLF}
M. Brahimi, K. Boukhalfa, and A. Msaaf,
\newblock ``Deep Learning for Tomato Diseases: Classification and Symptoms Visualization,''
\newblock {\em Applied Artificial Intelligence}, vol. 31, no. 4, pp. 299--315, 2017.

\bibitem{Ferentinos2018DeepLA}
K. P. Ferentinos,
\newblock ``Deep Learning Models for Plant Disease Detection and Diagnosis,''
\newblock {\em Computers and Electronics in Agriculture}, vol. 145, pp. 311--318, 2018.

\bibitem{Fuentes2017RobustDC}
A. Fuentes, S. Yoon, S. C. Kim, and D. S. Park,
\newblock ``A Robust Deep-Learning-Based Detector for Real-Time Tomato Plant Diseases and Pests Recognition,''
\newblock {\em Sensors}, vol. 17, no. 9, p. 2022, 2017.

\bibitem{Li2023GrapeViT}
L. Li, H. Yu, Y. Zhang, and Q. Li,
\newblock ``Grape Leaf Disease Identification Based on Improved Vision Transformer,''
\newblock {\em Computers and Electronics in Agriculture}, vol. 208, p. 107820, 2023.

\bibitem{Gao2022WheatRust}
Z. Gao, X. Liu, Y. Zhang, and H. Li,
\newblock ``Wheat Leaf Rust Detection Based on Hybrid CNN and Vision Transformer Model,''
\newblock {\em Plant Methods}, vol. 18, no. 1, pp. 1--12, 2022.

\bibitem{Liu2021SwinTH}
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
\newblock ``Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,''
\newblock {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 10012--10022, 2021.

\bibitem{Touvron2021TrainingDI}
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
\newblock ``Training Data-efficient Image Transformers \& Distillation through Attention,''
\newblock {\em Proceedings of the 38th International Conference on Machine Learning (ICML)}, pp. 10347--10357, 2021.

\bibitem{Wang2021PyramidVT}
W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao,
\newblock ``Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,''
\newblock {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 568--578, 2021.

\bibitem{Shorten2019Survey}
C. Shorten and T. M. Khoshgoftaar,
``A survey on Image Data Augmentation for Deep Learning,''
\emph{Journal of Big Data}, vol. 6, no. 1, pp. 1–48, 2019.

\bibitem{Tan2021Efficientnetv2}
M. Tan and Q. V. Le,
``EfficientNetV2: Smaller Models and Faster Training,''
in \emph{Proc. of ICML}, 2021.

\bibitem{Howard2020MobileNetV3}
A. Howard et al.,
``Searching for MobileNetV3,''
in \emph{Proc. of CVPR}, 2020.

\bibitem{Buslaev2020Albumentations}
A. Buslaev et al.,
``Albumentations: Fast and Flexible Image Augmentations,''
\emph{Information}, vol. 11, no. 2, p. 125, 2020.

\bibitem{Paszke2019PyTorch}
A. Paszke et al.,
``PyTorch: An Imperative Style, High-Performance Deep Learning Library,''
in \emph{Advances in Neural Information Processing Systems}, vol. 32, 2019.

\end{thebibliography}

\end{document}
